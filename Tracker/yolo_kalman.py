# -*- coding: utf-8 -*-
"""kalman.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jJlSLbdtRTAK2pKOOOWVlM3NWg1NYO6a
"""

import cv2
import numpy as np

class KalmanFilterTracker:
    def __init__(self):
        # Initialize Kalman Filter
        self.kf = cv2.KalmanFilter(4, 2)
        self.kf.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], np.float32)
        self.kf.transitionMatrix = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32)
        self.initialized = False
        # Increase process noise to adapt to dynamic movements faster
        self.kf.processNoiseCov = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32) * 0.1

        # Decrease measurement noise to trust the detections more
        self.kf.measurementNoiseCov = np.array([[1, 0], [0, 1]], np.float32) * 0.03

    def predict(self, pos):
        """
        Predicts the next position for the given detection.

        Args:
            pos: A tuple representing a detection in the format (x, y).
        """
        if pos is None:
            return None

        if not self.initialized:
            # Initialize Kalman Filter with the first detection
            self.kf.statePost = np.array([[pos[0]], [pos[1]], [0], [0]], np.float32)
            self.initialized = True
        else:
            # Update Kalman Filter with the new position
            measurement = np.array([[pos[0]], [pos[1]]], np.float32)
            self.kf.correct(measurement)

        # Predict the next state
        prediction = self.kf.predict()
        return (float(prediction[0]), float(prediction[1]))

# Example long array of positions
array_of_pos = [
    (100, 100), (105, 102), (110, 104), (115, 108), (120, 110), (125, 112),
    (130, 115), (135, 118), (140, 120), (145, 123), (150, 125), (155, 128),
    (160, 130), (165, 133), (170, 136), (175, 138), (180, 141), (185, 143),
    (190, 146), (195, 148), (200, 150), (205, 153), (210, 155), (215, 158),
    (220, 160), (225, 163), (230, 165), (235, 168), (240, 170), (245, 173),
    (250, 175), (255, 178), (260, 180), (265, 183), (270, 185), (275, 188)
]

# Create Kalman filter tracker
tracker = KalmanFilterTracker()

# Loop through the positions and predict each one
for pos in array_of_pos:
    predicted_pos = tracker.predict(pos)
    print(f"Predicted position: {predicted_pos}")

from IPython.display import HTML
from base64 import b64encode

def play_video(filename):
  mp4 = open(filename,'rb').read()
  data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
  return HTML("""
  <video width=400 controls>
        <source src="%s" type="video/mp4">
  </video>
  """ % data_url)

play_video('/content/tracked_duckvsman.mp4')
#

!pip install ultralytics

from ultralytics import YOLO
model = YOLO('/content/epoch104.pt')

cap = cv2.VideoCapture('/content/4K - Man Walking Alone - Free Copyright Video.mp4')

from google.colab.patches import cv2_imshow
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    results = model(frame)
    annotated_frame = results[0].plot()
    cv2_imshow(annotated_frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
cap.release()
cv2.destroyAllWindows()

fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # or use 'XVID' for .avi files
output_path = "tracked_duckvsman.mp4"
fps = cap.get(cv2.CAP_PROP_FPS)  # Get frames per second of the input video
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

tracker = KalmanFilterTracker()
prev_frame = None
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break


    if prev_frame is None:
        # Initialize prev_frame during the first iteration
        prev_frame = frame.copy()
        continue

    results = model(frame)
    annotated_frame = results[0].plot()
    for box in results[0].boxes.xywh:
        x_center = box[0] + box[2] / 2
        y_center = box[1] + box[3] / 2
        # Predict the center (x_center, y_center) of the bounding box using Kalman Filter
        pred_x_center, pred_y_center = tracker.predict((x_center, y_center))

        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
        curr_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        flow = cv2.calcOpticalFlowFarneback(prev_gray, curr_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

        # Calculate the average flow (motion) in the image
        avg_flow = np.mean(flow, axis=(0, 1))

        # Adjust the Kalman filter prediction by the estimated camera movement
        pred_x_center += avg_flow[0]
        pred_y_center += avg_flow[1]

        # Calculate the top-left corner of the predicted bounding box
        pred_x1 = int(pred_x_center - box[2] / 2)  # box[2] is width
        pred_y1 = int(pred_y_center - box[3] / 2)  # box[3] is height

        # Calculate the bottom-right corner of the predicted bounding box
        pred_x2 = int(pred_x_center + box[2] / 2)
        pred_y2 = int(pred_y_center + box[3] / 2)


        # Draw the predicted bounding box on the frame
        cv2.rectangle(annotated_frame, (pred_x1, pred_y1), (pred_x2, pred_y2), (0, 255, 0), 2)
        prev_frame = frame.copy()
        # Write the frame with the predicted bounding box to the output video
    out.write(annotated_frame)
    cv2_imshow(annotated_frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
cap.release()
cv2.destroyAllWindows()